센서로 들어오는 모든 데이터를 분류할 수 있음
세그멘테이션 : 들어온 데이터를 고정된 크기로 자를 수 있음. (fixed size 혹은 variable size)

window : 잘라진 데이터 그 자체(그룹핑)
오버래핑 : 각각의 window를 겹치게 하는 작업. 겹친 정도를 정할 수 있음(50%, 25%). 문제에 마다 모두 다름.

기본적으로 모델을 만들 때는 subject wise로 만들어야한다
subject wise : 사람별로 구분해서 트레인과 테스트 데이터를 나눈다.

1. 데이터 분할 (Data split)
일반적으로는 train/test를 무작위로 나눠.
그런데 “subject-wise split”은 한 사람이 가진 모든 데이터를 통째로 학습 세트나 테스트 세트에만 넣는 방식이야.
이렇게 하면 같은 사람의 데이터가 학습/테스트에 동시에 섞이지 않아서 모델이 특정 사람의 특성을 외워서 잘 맞추는 “치팅”을 막을 수 있어.
  
2. 평가 방식 (Evaluation)
예를 들어 행동 인식이나 생체 신호 인식(HAR, EEG, ECG 등)에서 “subject-wise accuracy”라고 하면, 각 사람 별로 모델 성능을 구해서 평균 내는 방식을 뜻해.
이렇게 하면 특정 소수 사용자에게만 성능이 높게 나오는 편향을 줄일 수 있어.
  
3. 대조: sample-wise / record-wise
sample-wise: 데이터를 그냥 샘플 단위로 랜덤 섞어서 train/test 분리. (subject가 섞일 수 있음)
subject-wise: 개인 단위로 끊어서 train/test 분리. (일종의 leave-one-subject-out cross-validation도 여기에 속해)
  
cross valudation(교차 검증)을 해야함
 - 데이터를 여러 번 나누어서 학습/검증을 반복하는 평가 방법이야.
한 번만 train/test로 나누면 결과가 우연(데이터 분할 방식)에 따라 달라질 수 있는데, cross-validation은 그걸 줄여서 모델 성능을 더 안정적이고 공정하게 평가해줘.

K-fold cross-validation
데이터를 K등분 → 한 fold는 validation, 나머지는 train → K번 반복.
(예: 5-fold CV → 데이터 5등분 → 5번 학습·검증 → 평균 성능 계산)

Leave-One-Out CV (LOOCV)
데이터 개수가 n이면 fold = n → 즉, 한 번에 하나의 샘플만 validation으로, 나머지는 train으로 사용.
→ 데이터가 적을 때 자주 씀.

Stratified K-fold CV
분류 문제에서 클래스 비율이 유지되도록 fold를 나누는 방식.
(예: 불균형 데이터셋에서 클래스 분포를 유지하면서 나눔)

고정길이 윈도우의 장점과 단점.
- 장점
하나의 활동 구간에서 여러 슬라이딩 윈도우를 추출할 수 있어, 데이터의 양이 크게 증가합니다.
- 단점
고정된 오버랩 비율이 모든 활동 구간에 최적은 아니기 때문에, 비슷한 데이터가 많이 생겨 오버피팅 위험이 커질 수 있고, 반대로 일반화 성능이 낮아질 수 있습니다.

슬라이딩 윈도우의 사이즈가 크고 작을 떄의 장단점
1. 윈도우 사이즈 (Window Size)

** 크기가 클 때
- 장점
더 많은 시계열 정보를 포함 → 활동 패턴을 충분히 포착 가능.
활동 전환점(transition)에도 더 강건.
잡음(노이즈)에 덜 민감 → 평균화 효과.

- 단점
실시간성 떨어짐 (긴 구간을 모아야 분류 가능).
빠른 활동(걷기 ↔ 뛰기) 변화를 놓칠 수 있음.
입력 차원 증가 → 모델 복잡도와 학습 시간 상승.

** 크기가 작을 때
- 장점
반응 속도 빠름 (실시간 시스템에 유리).
활동 전환 구간을 더 민감하게 탐지.
연산량 줄어듦.

- 단점
시계열 문맥 정보 부족 → 분류 정확도 저하 가능.
노이즈에 취약 → 변동성 커짐.

2. 오버래핑 (Overlap)

** 오버래핑 비율이 클 때 (예: 75%~90%)
- 장점
윈도우 간 데이터 중복 → 더 많은 학습 샘플 확보.
활동 시작·종료 시점 탐지에 유리.
모델이 더 부드러운 의사결정을 함 (시간 연속성 반영).

- 단점
데이터셋 크기 폭발적 증가 → 학습/추론 비용 증가.
중복 샘플 과다 → 과적합 위험.

** 오버래핑 비율이 작을 때 (예: 0~25%)
- 장점
연산량 감소, 메모리 효율적.
데이터 중복 줄어 → 과적합 위험 낮음.

- 단점
활동 구간 경계(transition)를 놓칠 수 있음.
샘플 수 부족 → 작은 데이터셋에서는 성능 저하.

3. 종합적으로

윈도우 크기: 일반적으로 2–5초(50~200 sample, 센서 주파수에 따라 다름)가 많이 사용됨.

오버래핑 비율: 50% 정도가 균형점으로 자주 선택됨.

실시간/저지연 → 작은 윈도우 + 낮은 오버랩.

오프라인/고정밀 → 큰 윈도우 + 높은 오버랩.

ps.
슬라이딩 윈도우 크기는 정확도와 연산 비용 간 트레이드오프를 만든다.
작은 윈도우 → 짧은 활동에 유리, 계산 부담 증가.
큰 윈도우 → 계산 효율적, 하지만 세밀한 인식 저하.
CNN-LSTM이 가장 효과적이며, 20초 윈도우 + 오버랩 방식이 최적의 균형.
