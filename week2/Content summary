선형 회귀와 로지스틱 회귀의 차이점 : 선형 회귀는 Regression, 로지스틱은 classification.

K-means와 KNN의 차이점 : KNN은 지도학습, K-means는 un지도학습

activation function의 역할 : 
 - 비선형성(Non-linearity) 부여
신경망이 단순한 선형 결합만 한다면, 아무리 층을 많이 쌓아도 결국 선형 회귀랑 같은 모델이 돼요.
→ 활성화 함수는 비선형성을 추가해서 복잡한 패턴(곡선, 고차원 데이터 등)을 학습할 수 있게 합니다.
 - 신호 조절 (squashing)
입력값을 특정 범위로 압축해서 너무 큰 값이나 음수가 다음 층으로 전달되는 걸 막습니다.
→ 이를 통해 학습 안정성과 수렴 속도를 높여줍니다.

경사하강법 3가지 종류 : 
1. 배치 경사하강법 (Batch Gradient Descent, BGD)
전체 데이터셋을 사용해서 한 번의 업데이트를 수행.
 - 장점 :
전역 최적화(Global Minimum)에 더 안정적으로 수렴
업데이트 방향이 정확
 - 단점 : 
데이터가 크면 연산량이 매우 큼 → 느림
메모리 부담 큼

2. 확률적 경사하강법 (Stochastic Gradient Descent, SGD) : 
데이터 하나(샘플 1개)로 매번 파라미터를 업데이트.
 - 장점 :
계산이 빠르고, 온라인 학습 가능
지역 최적해(Local Minimum)에서 빠져나올 가능성이 큼 (랜덤성 덕분)
 - 단점 : 
최적화 경로가 노이즈 많음 → 수렴이 불안정
너무 요동치면 수렴 못 하고 튈 수도 있음

3. 미니배치 경사하강법 (Mini-batch Gradient Descent, MBGD)
일부 데이터(예: 32, 64, 128개)를 묶어 업데이트.
 - 장점:
연산 효율 ↑ (GPU 병렬 연산에 적합)
SGD보다 덜 불안정하고, BGD보다 빠름
현재 딥러닝에서 가장 표준적으로 쓰임
 - 단점 : 
배치 크기 선택이 중요 (너무 작으면 불안정, 너무 크면 느림)

전이 학습 : 우리가 학습했던 weight와 bias를 가져다가 쓰자.

1D 시계열을 2D에 넣어주기 위해서 shape를 바꿔주는 것.
RNN을 쓰는 이유 : 이전 데이터를 반영해서 새롭게 하는 것(순환데이터).
